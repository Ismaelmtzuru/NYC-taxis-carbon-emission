{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping de los datos del dataset TLC Trip Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url donde se encuentran links\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# se verifica si la solicitud fue exitosa (codigo de estado 200)\n",
    "if response.status_code==200:\n",
    "    #parsear el condido html con beautiful soup\n",
    "    soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "    #Buscar todos los enlaces para el año 2021\n",
    "    year_section = soup.find('div',{'id':'faq2021'})\n",
    "\n",
    "    if year_section:\n",
    "        links = year_section.find_all('a')\n",
    "        for link in links:\n",
    "            # obtener url completa del enlace\n",
    "            link_url = urllib.parse.urljoin(url,link.get('href'))\n",
    "            \n",
    "            #se extrae el nombre del archivo del enlace\n",
    "            file_name = os.path.basename(link_url)\n",
    "\n",
    "            #se construye la ruta completa del archivo a ruta local\n",
    "            file_path = os.path.join(output_directory,file_name)\n",
    "            #se descarga el archivo si no existe aun (el script esta pensado para ser re-ejecutado)\n",
    "            if not os.path.exists(file_path):\n",
    "                response_file = requests.get(link_url)\n",
    "\n",
    "                if response_file.status_code == 200:\n",
    "                    with open(file_path,'wb') as file:\n",
    "                        file.write(response_file.content)\n",
    "                    print(f'archivo descargado: {file_path}')\n",
    "                else:\n",
    "                    print(f'error al descargar el archivo desde {link_url}. codigo de estado: {response_file.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos del dataset TLC Trip Record Data\n",
    "\n",
    "### Funciones usadas para persistir los datos en la Base de datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_esquema_tabla(nombre_tabla,tabla_existente=False):\n",
    "    if not tabla_existente:\n",
    "        if 'green' in nombre_tabla:\n",
    "            sql_command = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS green (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                VendorID INTEGER NOT NULL,\n",
    "                lpep_pickup_datetime DATETIME,\n",
    "                lpep_dropoff_datetime DATETIME,\n",
    "                store_and_fwd_flag VARCHAR(4),\n",
    "                RatecodeID INTEGER,\n",
    "                PULocationID INTEGER,\n",
    "                DOLocationID INTEGER,\n",
    "                passenger_count INTEGER,\n",
    "                trip_distance FLOAT,\n",
    "                fare_amount FLOAT,\n",
    "                extra FLOAT,\n",
    "                mta_tax FLOAT,\n",
    "                tip_amount FLOAT,\n",
    "                tolls_amount FLOAT,\n",
    "                improvement_surcharge FLOAT,\n",
    "                total_amount FLOAT,\n",
    "                payment_type INTEGER,\n",
    "                trip_type INTEGER,\n",
    "                congestion_surcharge FLOAT\n",
    "            )\n",
    "            \"\"\"\n",
    "        elif 'yellow' in nombre_tabla:\n",
    "            sql_command = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS yellow (\n",
    "                id INTEGER PRIMARY KEY  AUTOINCREMENT,\n",
    "                VendorID INTEGER NOT NULL,\n",
    "                tpep_pickup_datetime DATETIME,\n",
    "                tpep_dropoff_datetime DATETIME,\n",
    "                passenger_count INTEGER,\n",
    "                trip_distance FLOAT,\n",
    "                RatecodeID INTEGER,\n",
    "                store_and_fwd_flag VARCHAR(4),\n",
    "                PULocationID INTEGER,\n",
    "                DOLocationID INTEGER,\n",
    "                payment_type INTEGER,\n",
    "                fare_amount FLOAT,\n",
    "                extra FLOAT,\n",
    "                mta_tax FLOAT,\n",
    "                tip_amount FLOAT,\n",
    "                tolls_amount FLOAT,\n",
    "                improvement_surcharge FLOAT,\n",
    "                total_amount FLOAT,\n",
    "                congestion_surcharge FLOAT,\n",
    "                Airport_fee INTEGER\n",
    "            )\n",
    "            \"\"\"\n",
    "        elif 'fhvhv' in nombre_tabla:\n",
    "            sql_command = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fhvhv(\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                hvfhs_license_num VARCHAR(12) NOT NULL,\n",
    "                dispatching_base_num VARCHAR(12) NOT NULL,\n",
    "                originating_base_num VARCHAR(12) NOT NULL,\n",
    "                request_datetime DATETIME,\n",
    "                on_scene_datetime DATETIME,\n",
    "                pickup_datetime DATETIME,\n",
    "                dropoff_datetime DATETIME,\n",
    "                PULocationID INTEGER,\n",
    "                DOLocationID INTEGER,\n",
    "                trip_miles FLOAT,\n",
    "                trip_time INTEGER,\n",
    "                base_passenger_fare FLOAT,\n",
    "                tolls FLOAT,\n",
    "                bcf FLOAT,\n",
    "                sales_tax FLOAT,\n",
    "                congestion_surcharge FLOAT,\n",
    "                airport_fee FLOAT,\n",
    "                tips FLOAT,\n",
    "                driver_pay FLOAT,\n",
    "                shared_request_flag VARCHAR(1),\n",
    "                shared_match_flag VARCHAR(1),\n",
    "                access_a_ride_flag VARCHAR(1),\n",
    "                wav_request_flag VARCHAR(1),\n",
    "                wav_match_flag VARCHAR(1))\"\"\"\n",
    "            \n",
    "        elif 'fhv' in nombre_tabla:\n",
    "            sql_command= \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fhv (\n",
    "                id INTEGER PRIMARY KEY  AUTOINCREMENT,\n",
    "                dispatching_base_num VARCHAR(12) NOT NULL,\n",
    "                pickup_datetime DATETIME,\n",
    "                dropOff_datetime DATETIME,\n",
    "                PUlocationID INTEGER,\n",
    "                DOlocationID INTEGER,\n",
    "                Affiliated_base_number VARCHAR(12) NOT NULL\n",
    "                )\n",
    "                \"\"\"\n",
    "    return sql_command\n",
    "\n",
    "# Funcion para cargar archivos\n",
    "def carga_sqlite(nombre_tabla,comando_sql,ruta_limpia,archivo):\n",
    "    # comando SQL para la creación de la tabla\n",
    "\n",
    "    # se conecta a la base de datos SQLlite\n",
    "    ruta_bd = 'C:\\\\Users\\\\Usuario\\\\Documents\\\\RENE\\\\Personal\\\\Conocimiento\\\\Capacitacion\\\\DataScience\\\\Henry\\\\ProyecctoFinal\\\\PT04\\\\Sprint2\\\\Data\\\\BD\\\\bd.db'\n",
    "    with sqlite3.connect(ruta_bd) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        #si la tabla no existe, crearla con el esquema definido\n",
    "        cursor.execute(comando_sql)\n",
    "\n",
    "        df = pd.read_parquet(os.path.join(ruta_limpia,archivo))\n",
    "\n",
    "        #convertir el dataframe a una tabla SQLite\n",
    "        df.to_sql(nombre_tabla,conn,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url donde se encuentran links\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "output_directory = 'C:\\\\Users\\\\Usuario\\\\Documents\\\\RENE\\\\Personal\\\\Conocimiento\\\\Capacitacion\\\\DataScience\\\\Henry\\\\ProyecctoFinal\\\\PT04\\\\Sprint2\\\\Data\\\\raw\\\\TripData'\n",
    "response = requests.get(url)\n",
    "ruta_limpia = 'C:\\\\Users\\\\Usuario\\\\Documents\\\\RENE\\\\Personal\\\\Conocimiento\\\\Capacitacion\\\\DataScience\\\\Henry\\\\ProyecctoFinal\\\\PT04\\\\Sprint2\\\\Data\\\\clean\\\\TripData'\n",
    "\n",
    "\n",
    "descargados = os.listdir(output_directory)\n",
    "\n",
    "# Iterar sobre los archivos descargados para limpiar y guardar en otra carpeta\n",
    "for archivo in descargados:\n",
    "    archivo_ruta = os.path.join(output_directory, archivo)\n",
    "    archivo_ruta_limpia = os.path.join(ruta_limpia, archivo)\n",
    "    \n",
    "    # Se verifica si el archivo existe para no volver a limpiar\n",
    "    if not os.path.exists(archivo_ruta_limpia):\n",
    "        #obtener tipo de arcvhio\n",
    "        tipo_archivo = archivo.split('_')[0].lower()\n",
    "            \n",
    "        # Se realiza la limieza según el tipo de archivo (green,yellow,fhv,fhvhv)\n",
    "        if 'green' == tipo_archivo:\n",
    "            df = pd.read_parquet(archivo_ruta) \n",
    "            # Realizar las operaciones de limpieza para Green trip data\n",
    "            df.drop('ehail_fee', axis=1, inplace=True)\n",
    "            df['store_and_fwd_flag'].fillna('N', inplace=True)\n",
    "            df['RatecodeID'].fillna(1, inplace=True)\n",
    "            df['RatecodeID'] = df['RatecodeID'].replace(99.0, 1.0)\n",
    "            redondeo = math.floor(df['passenger_count'].mean())\n",
    "            df['passenger_count'].fillna(redondeo, inplace=True)\n",
    "            df = df[df['trip_distance'] <= 250]\n",
    "            df['payment_type'].fillna(5, inplace=True)\n",
    "            df['trip_type'].fillna(1, inplace=True)\n",
    "            df['congestion_surcharge'].fillna(0, inplace=True)\n",
    "            df['RatecodeID'] = df['RatecodeID'].astype(int)\n",
    "            df['passenger_count'] = df['passenger_count'].astype(int)\n",
    "            df['payment_type'] = df['payment_type'].astype(int)\n",
    "            df['trip_type'] = df['trip_type'].astype(int)\n",
    "\n",
    "\n",
    "        elif 'yellow' == tipo_archivo:\n",
    "            df = pd.read_parquet(archivo_ruta)  \n",
    "\n",
    "            # Realizar las operaciones de limpieza para Yellow trip data\n",
    "            redondeo = math.floor(df['passenger_count'].mean())\n",
    "            df['passenger_count'].fillna(redondeo, inplace=True)\n",
    "            df['RatecodeID'].fillna(1, inplace=True)\n",
    "            df['RatecodeID'] = df['RatecodeID'].replace(99.0, 1.0)\n",
    "            df['store_and_fwd_flag'] = df['store_and_fwd_flag'].fillna('N')\n",
    "            df['payment_type'].replace(0, 5, inplace=True)\n",
    "            df['congestion_surcharge'].fillna(0, inplace=True)\n",
    "            if 'airpor_fee' in df.columns:\n",
    "                df['airport_fee'].fillna(0,inplace=True)\n",
    "            df['RatecodeID'] = df['RatecodeID'].astype(int)\n",
    "            df['passenger_count'] = df['passenger_count'].astype(int)\n",
    "            df['PULocationID'] = df['PULocationID'].astype(int)\n",
    "            df['DOLocationID'] = df['DOLocationID'].astype(int)\n",
    "        elif \"fhvhv\" == tipo_archivo:\n",
    "            df = pd.read_parquet(archivo_ruta)  \n",
    "\n",
    "            # Realizar las operaciones de limpieza para FHVHV TRIPDATA\n",
    "            df['originating_base_num'] = df['originating_base_num'].str.strip()\n",
    "            comun = dict(df['originating_base_num'].value_counts())\n",
    "            valor = list(comun.keys())\n",
    "            df['originating_base_num'].fillna(valor[0], inplace=True)\n",
    "            # Rellenar valores nulos en access_a_ride_flag\n",
    "            df['access_a_ride_flag'].replace(\" \",'N',inplace=True)\n",
    "        elif 'fhv' == tipo_archivo:\n",
    "            df = pd.read_parquet(archivo_ruta)\n",
    "\n",
    "            # Realizar las operaciones de limpieza para FHV TRIPDATA\n",
    "            df.drop(df[(df['PUlocationID'].isnull()) & (df['DOlocationID'].isnull())].index, inplace=True)\n",
    "            df.drop(df[df['PUlocationID'].isnull()].index, inplace=True)\n",
    "            df.drop(df[df['DOlocationID'].isnull()].index, inplace=True)\n",
    "            df.drop('SR_Flag', axis=1, inplace=True)\n",
    "            df['Affiliated_base_number'] = df['Affiliated_base_number'].str.strip()\n",
    "            comun = dict(df['Affiliated_base_number'].value_counts())\n",
    "            valor = list(comun.keys())\n",
    "            df['Affiliated_base_number'] = df['Affiliated_base_number'].replace(\"\", valor[0])\n",
    "            df['PUlocationID'] = df['PUlocationID'].astype(int)\n",
    "            df['DOlocationID'] = df['DOlocationID'].astype(int)\n",
    "\n",
    "        # Guardar en la carpeta \"clean\"\n",
    "        df.to_parquet(os.path.join(ruta_limpia, archivo), index=False)\n",
    "        #se carga el esquema dependiendo del nombre\n",
    "        sql_esquema = sql_esquema_tabla(tipo_archivo)\n",
    "        #se carga a la base de datos\n",
    "        carga_sqlite(tipo_archivo,sql_esquema,ruta_limpia,archivo)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping del dataset NYC Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados exitosamente en aire.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "url_calidad_aire = \"https://data.cityofnewyork.us/resource/c3uy-2p5r.json\"\n",
    "destino = \"..\\\\Data\\\\aire\"\n",
    "archivo = 'calidad_aire.json'\n",
    "# Realizar una solicitud GET a la API\n",
    "response = requests.get(url_calidad_aire)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa (código de estado 200)\n",
    "if response.status_code == 200:\n",
    "    # Los datos están en formato JSON, puedes acceder a ellos utilizando el método json()\n",
    "    data = response.json()\n",
    "    \n",
    "    #si la carpeta no existe, se crea\n",
    "    if not os.path.exists(destino):\n",
    "        os.makedirs(destino)\n",
    "\n",
    "    #construir la ruta completa del archivo dentro de la carpeta destino\n",
    "    ruta_aire = os.path.join(destino,archivo)    \n",
    "\n",
    "    # Escribir los datos en un archivo local (por ejemplo, en formato JSON)\n",
    "    with open(ruta_aire, \"w\") as archivo:\n",
    "        json.dump(data, archivo)\n",
    "    print(f\"Datos guardados exitosamente en {destino}.\")\n",
    "else:\n",
    "    print(f\"Error al descargar datos. Código de estado: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
